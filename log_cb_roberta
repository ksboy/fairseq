root@48153f4f99fc:/data/houwei/fairseq# bash cb.sh
Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, arch='roberta_large', attention_dropout=0.1, best_checkpoint_metric='f1', bpe=None, bucket_cap_mb=25, clip_norm=0.0, cpu=False, criterion='sentence_prediction', curriculum=0, data='CB-bin/', dataset_impl=None, ddp_backend='c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layers=24, end_learning_rate=0.0, find_unused_parameters=False, fix_batches_to_gpus=False, force_anneal=None, fp16=True, fp16_init_scale=4, fp16_scale_tolerance=0.0, fp16_scale_window=128, init_token=0, keep_interval_updates=-1, keep_last_epochs=-1, log_format=None, log_interval=1000, lr=[1e-05], lr_scheduler='polynomial_decay', max_epoch=10, max_positions=512, max_sentences=16, max_sentences_valid=16, max_tokens=4400, max_tokens_valid=4400, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_shuffle=False, num_classes=3, num_workers=1, optimizer='adam', optimizer_overrides='{}', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, regression_target=False, required_batch_size_multiple=1, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='../roberta.large.mnli/model.pt', save_dir='../roberta.large.mnli/', save_interval=1, save_interval_updates=0, save_predictions=None, seed=1, sentence_avg=False, separator_token=2, skip_invalid_size_inputs_valid_test=False, task='sentence_prediction', tbmf_wrapper=False, tensorboard_logdir='', threshold_loss_scale=1.0, tokenizer=None, total_num_update=200, train_subset='train', truncate_sequence=False, update_freq=[1], use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=12, weight_decay=0.1)
| [input] dictionary: 50265 types
| [label] dictionary: 9 types
| loaded 56 examples from: CB-bin/input0/valid
| loaded 56 examples from: CB-bin/input1/valid
| loaded 56 examples from: CB-bin/label/valid
| Loaded valid with #samples: 56
RobertaModel(
  (decoder): RobertaEncoder(
    (sentence_encoder): TransformerSentenceEncoder(
      (embed_tokens): Embedding(50265, 1024, padding_idx=1)
      (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)
      (layers): ModuleList(
        (0): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (12): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (13): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (14): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (15): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (16): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (17): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (18): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (19): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (20): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (21): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (22): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
        (23): TransformerSentenceEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        )
      )
      (emb_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
    )
    (lm_head): RobertaLMHead(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
    )
  )
  (classification_heads): ModuleDict(
    (sentence_classification_head): RobertaClassificationHead(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (dropout): Dropout(p=0.0)
      (out_proj): Linear(in_features=1024, out_features=3, bias=True)
    )
  )
)
| model roberta_large, criterion SentencePredictionCriterion
| num. model params: 356463708 (num. trained: 356463708)
| training on 1 GPUs
| max tokens per GPU = 4400 and max sentences per GPU = 16
Overwriting classification_heads.sentence_classification_head.dense.weight
Overwriting classification_heads.sentence_classification_head.dense.bias
Overwriting classification_heads.sentence_classification_head.out_proj.weight
Overwriting classification_heads.sentence_classification_head.out_proj.bias
| loaded checkpoint ../roberta.large.mnli/../roberta.large.mnli/model.pt (epoch 0 @ 0 updates)
| WARNING: your device does NOT support faster training with --fp16, please switch to FP32 which is likely to be faster
| loading train data for epoch 0
| loaded 250 examples from: CB-bin/input0/train
| loaded 250 examples from: CB-bin/input1/train
| loaded 250 examples from: CB-bin/label/train
| Loaded train with #samples: 250
| epoch 001:   0%|                                                                                    | 0/16 [00:00<?, ?it/s]/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
| epoch 001:   6%| | 1/16 [00:02<00:31,  2.11s/it, loss=1.667, nll_loss=0.017, ppl=1.01, wps=574, ups=0, wpb=1556.000, bsz=16.000, num_updates=1, lr=8.33333e-07, gnorm=14.186, clip=0.000, oom=0.000, loss_scale=4.000, wall=3, train_wall=2, accuracy=0.| epoch 001:  12%|▏| 2/16 [00:04<00:28,  2.05s/it, loss=1.576, nll_loss=0.019, ppl=1.01, wps=543, ups=0, wpb=1293.500, bsz=16.000, num_updates=2, lr=1.66667e-06, gnorm=11.702, clip=0.000, oom=0.000, loss_scale=4.000, wall=5, train_wall=4, accuracy=0.| epoch 001:  19%|▏| 3/16 [00:05<00:26,  2.02s/it, loss=1.566, nll_loss=0.019, ppl=1.01, wps=649, ups=0, wpb=1353.667, bsz=16.000, num_updates=3, lr=2.5e-06, gnorm=11.424, clip=0.000, oom=0.000, loss_scale=4.000, wall=7, train_wall=6, accuracy=0.4375| epoch 001:  25%|▎| 4/16 [00:08<00:24,  2.04s/it, loss=1.573, nll_loss=0.018, ppl=1.01, wps=688, ups=0, wpb=1414.000, bsz=16.000, num_updates=4, lr=3.33333e-06, gnorm=10.878, clip=0.000, oom=0.000, loss_scale=4.000, wall=9, train_wall=8, accuracy=0.| epoch 001:  31%|▎| 5/16 [00:10<00:22,  2.02s/it, loss=1.570, nll_loss=0.018, ppl=1.01, wps=682, ups=0, wpb=1391.800, bsz=16.000, num_updates=5, lr=4.16667e-06, gnorm=11.406, clip=0.000, oom=0.000, loss_scale=4.000, wall=11, train_wall=10, accuracy=| epoch 001:  38%|▍| 6/16 [00:11<00:18,  1.88s/it, loss=1.532, nll_loss=0.018, ppl=1.01, wps=711, ups=0, wpb=1382.167, bsz=16.000, num_updates=6, lr=5e-06, gnorm=11.704, clip=0.000, oom=0.000, loss_scale=4.000, wall=12, train_wall=11, accuracy=0.4687| epoch 001:  44%|▍| 7/16 [00:13<00:17,  1.96s/it, loss=1.517, nll_loss=0.018, ppl=1.01, wps=687, ups=0, wpb=1362.714, bsz=16.000, num_updates=7, lr=5.83333e-06, gnorm=11.509, clip=0.000, oom=0.000, loss_scale=4.000, wall=14, train_wall=14, accuracy=| epoch 001:  50%|▌| 8/16 [00:15<00:14,  1.86s/it, loss=1.485, nll_loss=0.018, ppl=1.01, wps=697, ups=1, wpb=1348.625, bsz=16.000, num_updates=8, lr=6.66667e-06, gnorm=11.353, clip=0.000, oom=0.000, loss_scale=4.000, wall=16, train_wall=15, accuracy=| epoch 001:  56%|▌| 9/16 [00:17<00:12,  1.83s/it, loss=1.437, nll_loss=0.017, ppl=1.01, wps=690, ups=1, wpb=1322.889, bsz=16.000, num_updates=9, lr=7.5e-06, gnorm=11.126, clip=0.000, oom=0.000, loss_scale=4.000, wall=18, train_wall=17, accuracy=0.56| epoch 001:  62%|▋| 10/16 [00:18<00:10,  1.77s/it, loss=1.391, nll_loss=0.017, ppl=1.01, wps=703, ups=1, wpb=1324.300, bsz=16.000, num_updates=10, lr=8.33333e-06, gnorm=10.822, clip=0.000, oom=0.000, loss_scale=4.000, wall=19, train_wall=19, accurac| epoch 001:  69%|▋| 11/16 [00:20<00:09,  1.90s/it, loss=1.359, nll_loss=0.016, ppl=1.01, wps=694, ups=1, wpb=1330.000, bsz=16.000, num_updates=11, lr=9.16667e-06, gnorm=10.516, clip=0.000, oom=0.000, loss_scale=4.000, wall=22, train_wall=21, accurac| epoch 001:  75%|▊| 12/16 [00:23<00:07,  2.00s/it, loss=1.319, nll_loss=0.016, ppl=1.01, wps=695, ups=1, wpb=1349.583, bsz=16.000, num_updates=12, lr=1e-05, gnorm=10.357, clip=0.000, oom=0.000, loss_scale=4.000, wall=24, train_wall=23, accuracy=0.65| epoch 001:  81%|▊| 13/16 [00:24<00:05,  1.80s/it, loss=1.286, nll_loss=0.016, ppl=1.01, wps=698, ups=1, wpb=1324.077, bsz=16.000, num_updates=13, lr=9.94681e-06, gnorm=10.306, clip=0.000, oom=0.000, loss_scale=4.000, wall=25, train_wall=24, accurac| epoch 001:  88%|▉| 14/16 [00:25<00:03,  1.54s/it, loss=1.264, nll_loss=0.015, ppl=1.01, wps=698, ups=1, wpb=1275.714, bsz=15.571, num_updates=14, lr=9.89362e-06, gnorm=10.432, clip=0.000, oom=0.000, loss_scale=4.000, wall=26, train_wall=25, accurac| epoch 001:  94%|▉| 15/16 [00:27<00:01,  1.61s/it, loss=1.234, nll_loss=0.015, ppl=1.01, wps=696, ups=1, wpb=1269.467, bsz=15.600, num_updates=15, lr=9.84043e-06, gnorm=10.227, clip=0.000, oom=0.000, loss_scale=4.000, wall=28, train_wall=27, accurac| epoch 001: 100%|█| 16/16 [00:29<00:00,  1.73s/it, loss=1.200, nll_loss=0.015, ppl=1.01, wps=699, ups=1, wpb=1282.625, bsz=15.625, num_updates=16, lr=9.78723e-06, gnorm=9.962, clip=0.000, oom=0.000, loss_scale=4.000, wall=30, train_wall=29, accuracy                                                                                                                                                                                                                                                          | epoch 001 | loss 1.200 | nll_loss 0.015 | ppl 1.01 | wps 698 | ups 1 | wpb 1282.625 | bsz 15.625 | num_updates 16 | lr 9.78723e-06 | gnorm 9.962 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 30 | train_wall 29 | accuracy 0.712265
extra_meters:  defaultdict(<function validate.<locals>.<lambda> at 0x7f26667b6d08>, {'accuracy': <fairseq.meters.AverageMeter object at 0x7f266675e908>})
0.5993265993265994
accuracy <fairseq.meters.AverageMeter object at 0x7f266675e908>
f1 <fairseq.meters.AverageMeter object at 0x7f26665acba8>
| epoch 001 | valid on 'valid' subset | loss 0.658 | nll_loss 0.007 | ppl 1.00 | num_updates 16 | accuracy 0.859375 | f1 0.599327
| saved checkpoint ../roberta.large.mnli/checkpoint1.pt (epoch 1 @ 16 updates) (writing took 81.32717800140381 seconds)
| epoch 002:   6%| | 1/16 [00:01<00:28,  1.87s/it, loss=0.463, nll_loss=0.007, ppl=1.00, wps=13, ups=0, wpb=1117.000, bsz=16.000, num_updates=17, lr=9.73404e-06, gnorm=9.974, clip=0.000, oom=0.000, loss_scale=4.000, wall=116, train_wall=31, accuracy=| epoch 002:  12%|▏| 2/16 [00:03<00:25,  1.83s/it, loss=0.565, nll_loss=0.008, ppl=1.01, wps=679, ups=0, wpb=1149.500, bsz=16.000, num_updates=18, lr=9.68085e-06, gnorm=7.428, clip=0.000, oom=0.000, loss_scale=4.000, wall=117, train_wall=33, accuracy| epoch 002:  19%|▏| 3/16 [00:05<00:24,  1.85s/it, loss=0.558, nll_loss=0.008, ppl=1.01, wps=609, ups=0, wpb=1110.000, bsz=16.000, num_updates=19, lr=9.62766e-06, gnorm=8.527, clip=0.000, oom=0.000, loss_scale=4.000, wall=119, train_wall=34, accuracy| epoch 002:  25%|▎| 4/16 [00:07<00:23,  1.94s/it, loss=0.519, nll_loss=0.007, ppl=1.01, wps=597, ups=0, wpb=1144.000, bsz=16.000, num_updates=20, lr=9.57447e-06, gnorm=7.617, clip=0.000, oom=0.000, loss_scale=4.000, wall=122, train_wall=37, accuracy| epoch 002:  31%|▎| 5/16 [00:09<00:19,  1.76s/it, loss=0.491, nll_loss=0.007, ppl=1.00, wps=627, ups=0, wpb=1118.800, bsz=16.000, num_updates=21, lr=9.52128e-06, gnorm=6.889, clip=0.000, oom=0.000, loss_scale=4.000, wall=123, train_wall=38, accuracy| epoch 002:  38%|▍| 6/16 [00:11<00:18,  1.89s/it, loss=0.490, nll_loss=0.007, ppl=1.00, wps=629, ups=0, wpb=1163.500, bsz=16.000, num_updates=22, lr=9.46809e-06, gnorm=7.356, clip=0.000, oom=0.000, loss_scale=4.000, wall=125, train_wall=40, accuracy| epoch 002:  44%|▍| 7/16 [00:13<00:17,  1.91s/it, loss=0.459, nll_loss=0.006, ppl=1.00, wps=657, ups=0, wpb=1219.571, bsz=16.000, num_updates=23, lr=9.41489e-06, gnorm=7.762, clip=0.000, oom=0.000, loss_scale=4.000, wall=127, train_wall=42, accuracy| epoch 002:  50%|▌| 8/16 [00:14<00:12,  1.62s/it, loss=0.458, nll_loss=0.006, ppl=1.00, wps=659, ups=0, wpb=1148.000, bsz=15.250, num_updates=24, lr=9.3617e-06, gnorm=8.570, clip=0.000, oom=0.000, loss_scale=4.000, wall=128, train_wall=43, accuracy=| epoch 002:  56%|▌| 9/16 [00:16<00:12,  1.73s/it, loss=0.444, nll_loss=0.006, ppl=1.00, wps=672, ups=0, wpb=1184.222, bsz=15.333, num_updates=25, lr=9.30851e-06, gnorm=8.284, clip=0.000, oom=0.000, loss_scale=4.000, wall=130, train_wall=45, accuracy| epoch 002:  62%|▋| 10/16 [00:18<00:11,  1.83s/it, loss=0.455, nll_loss=0.006, ppl=1.00, wps=684, ups=0, wpb=1225.300, bsz=15.400, num_updates=26, lr=9.25532e-06, gnorm=8.214, clip=0.000, oom=0.000, loss_scale=4.000, wall=132, train_wall=47, accurac| epoch 002:  69%|▋| 11/16 [00:20<00:09,  1.98s/it, loss=0.434, nll_loss=0.005, ppl=1.00, wps=683, ups=0, wpb=1256.182, bsz=15.455, num_updates=27, lr=9.20213e-06, gnorm=9.474, clip=0.000, oom=0.000, loss_scale=4.000, wall=134, train_wall=49, accurac| epoch 002:  75%|▊| 12/16 [00:22<00:07,  1.87s/it, loss=0.424, nll_loss=0.005, ppl=1.00, wps=694, ups=0, wpb=1262.917, bsz=15.500, num_updates=28, lr=9.14894e-06, gnorm=9.256, clip=0.000, oom=0.000, loss_scale=4.000, wall=136, train_wall=51, accurac| epoch 002:  81%|▊| 13/16 [00:23<00:05,  1.78s/it, loss=0.479, nll_loss=0.006, ppl=1.00, wps=705, ups=0, wpb=1268.385, bsz=15.538, num_updates=29, lr=9.09574e-06, gnorm=9.850, clip=0.000, oom=0.000, loss_scale=4.000, wall=138, train_wall=53, accurac| epoch 002:  88%|▉| 14/16 [00:25<00:03,  1.83s/it, loss=0.452, nll_loss=0.006, ppl=1.00, wps=702, ups=0, wpb=1270.857, bsz=15.571, num_updates=30, lr=9.04255e-06, gnorm=9.463, clip=0.000, oom=0.000, loss_scale=4.000, wall=139, train_wall=54, accurac| epoch 002:  94%|▉| 15/16 [00:27<00:01,  1.89s/it, loss=0.444, nll_loss=0.005, ppl=1.00, wps=705, ups=0, wpb=1284.800, bsz=15.600, num_updates=31, lr=8.98936e-06, gnorm=9.113, clip=0.000, oom=0.000, loss_scale=4.000, wall=142, train_wall=56, accurac| epoch 002: 100%|█| 16/16 [00:29<00:00,  1.81s/it, loss=0.441, nll_loss=0.005, ppl=1.00, wps=709, ups=0, wpb=1282.625, bsz=15.625, num_updates=32, lr=8.93617e-06, gnorm=9.013, clip=0.000, oom=0.000, loss_scale=4.000, wall=143, train_wall=58, accurac                                                                                                                                                                                                                                                          | epoch 002 | loss 0.441 | nll_loss 0.005 | ppl 1.00 | wps 707 | ups 0 | wpb 1282.625 | bsz 15.625 | num_updates 32 | lr 8.93617e-06 | gnorm 9.013 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 143 | train_wall 58 | accuracy 0.915977
extra_meters:  defaultdict(<function validate.<locals>.<lambda> at 0x7f2666741bf8>, {'accuracy': <fairseq.meters.AverageMeter object at 0x7f266675e470>})
0.6121863799283154
accuracy <fairseq.meters.AverageMeter object at 0x7f266675e470>
f1 <fairseq.meters.AverageMeter object at 0x7f26665acef0>
| epoch 002 | valid on 'valid' subset | loss 0.364 | nll_loss 0.004 | ppl 1.00 | num_updates 32 | best_f1 0.612186 | accuracy 0.875 | f1 0.612186
^[[B^[[B^[[B^[[B^[[B^[[B^[[B| saved checkpoint ../roberta.large.mnli/checkpoint2.pt (epoch 2 @ 32 updates) (writing took 81.32959508895874 seconds)
| epoch 003:   6%| | 1/16 [00:02<00:31,  2.09s/it, loss=0.063, nll_loss=0.001, ppl=1.00, wps=18, ups=0, wpb=1556.000, bsz=16.000, num_updates=33, lr=8.88298e-06, gnorm=1.056, clip=0.000, oom=0.000, loss_scale=4.000, wall=229, train_wall=60, accuracy=| epoch 003:  12%|▏| 2/16 [00:04<00:29,  2.09s/it, loss=0.194, nll_loss=0.002, ppl=1.00, wps=764, ups=0, wpb=1575.500, bsz=16.000, num_updates=34, lr=8.82979e-06, gnorm=5.535, clip=0.000, oom=0.000, loss_scale=4.000, wall=231, train_wall=62, accuracy| epoch 003:  19%|▏| 3/16 [00:05<00:24,  1.87s/it, loss=0.191, nll_loss=0.002, ppl=1.00, wps=761, ups=0, wpb=1389.667, bsz=16.000, num_updates=35, lr=8.7766e-06, gnorm=4.532, clip=0.000, oom=0.000, loss_scale=4.000, wall=233, train_wall=64, accuracy=| epoch 003:  25%|▎| 4/16 [00:07<00:21,  1.77s/it, loss=0.316, nll_loss=0.004, ppl=1.00, wps=792, ups=0, wpb=1375.750, bsz=16.000, num_updates=36, lr=8.7234e-06, gnorm=5.669, clip=0.000, oom=0.000, loss_scale=4.000, wall=234, train_wall=65, accuracy=| epoch 003:  31%|▎| 5/16 [00:08<00:19,  1.81s/it, loss=0.290, nll_loss=0.004, ppl=1.00, wps=724, ups=0, wpb=1306.800, bsz=16.000, num_updates=37, lr=8.67021e-06, gnorm=5.151, clip=0.000, oom=0.000, loss_scale=4.000, wall=236, train_wall=67, accuracy| epoch 003:  38%|▍| 6/16 [00:10<00:18,  1.86s/it, loss=0.250, nll_loss=0.003, ppl=1.00, wps=710, ups=0, wpb=1306.167, bsz=16.000, num_updates=38, lr=8.61702e-06, gnorm=4.501, clip=0.000, oom=0.000, loss_scale=4.000, wall=238, train_wall=69, accuracy| epoch 003:  44%|▍| 7/16 [00:12<00:17,  1.92s/it, loss=0.225, nll_loss=0.003, ppl=1.00, wps=719, ups=0, wpb=1343.143, bsz=16.000, num_updates=39, lr=8.56383e-06, gnorm=4.165, clip=0.000, oom=0.000, loss_scale=4.000, wall=240, train_wall=71, accuracy| epoch 003:  50%|▌| 8/16 [00:15<00:16,  2.00s/it, loss=0.207, nll_loss=0.002, ppl=1.00, wps=705, ups=0, wpb=1348.625, bsz=16.000, num_updates=40, lr=8.51064e-06, gnorm=4.422, clip=0.000, oom=0.000, loss_scale=4.000, wall=242, train_wall=73, accuracy| epoch 003:  56%|▌| 9/16 [00:16<00:11,  1.69s/it, loss=0.205, nll_loss=0.002, ppl=1.00, wps=703, ups=0, wpb=1270.667, bsz=15.333, num_updates=41, lr=8.45745e-06, gnorm=4.214, clip=0.000, oom=0.000, loss_scale=4.000, wall=243, train_wall=74, accuracy| epoch 003:  62%|▋| 10/16 [00:17<00:10,  1.70s/it, loss=0.223, nll_loss=0.003, ppl=1.00, wps=700, ups=0, wpb=1261.800, bsz=15.400, num_updates=42, lr=8.40426e-06, gnorm=5.091, clip=0.000, oom=0.000, loss_scale=4.000, wall=245, train_wall=76, accurac| epoch 003:  69%|▋| 11/16 [00:19<00:09,  1.81s/it, loss=0.219, nll_loss=0.003, ppl=1.00, wps=702, ups=0, wpb=1281.636, bsz=15.455, num_updates=43, lr=8.35106e-06, gnorm=4.889, clip=0.000, oom=0.000, loss_scale=4.000, wall=247, train_wall=78, accurac| epoch 003:  75%|▊| 12/16 [00:22<00:07,  1.92s/it, loss=0.211, nll_loss=0.003, ppl=1.00, wps=689, ups=0, wpb=1278.667, bsz=15.500, num_updates=44, lr=8.29787e-06, gnorm=4.858, clip=0.000, oom=0.000, loss_scale=4.000, wall=249, train_wall=80, accurac| epoch 003:  81%|▊| 13/16 [00:24<00:05,  1.93s/it, loss=0.203, nll_loss=0.002, ppl=1.00, wps=694, ups=0, wpb=1293.692, bsz=15.538, num_updates=45, lr=8.24468e-06, gnorm=4.635, clip=0.000, oom=0.000, loss_scale=4.000, wall=251, train_wall=82, accurac| epoch 003:  88%|▉| 14/16 [00:25<00:03,  1.88s/it, loss=0.191, nll_loss=0.002, ppl=1.00, wps=690, ups=0, wpb=1281.071, bsz=15.571, num_updates=46, lr=8.19149e-06, gnorm=4.346, clip=0.000, oom=0.000, loss_scale=4.000, wall=253, train_wall=84, accurac| epoch 003:  94%|▉| 15/16 [00:27<00:01,  1.80s/it, loss=0.198, nll_loss=0.002, ppl=1.00, wps=699, ups=0, wpb=1284.800, bsz=15.600, num_updates=47, lr=8.1383e-06, gnorm=4.825, clip=0.000, oom=0.000, loss_scale=4.000, wall=255, train_wall=85, accuracy| epoch 003: 100%|█| 16/16 [00:29<00:00,  1.75s/it, loss=0.191, nll_loss=0.002, ppl=1.00, wps=703, ups=0, wpb=1282.625, bsz=15.625, num_updates=48, lr=8.08511e-06, gnorm=4.629, clip=0.000, oom=0.000, loss_scale=4.000, wall=256, train_wall=87, accurac                                                                                                                                                                                                                                                          | epoch 003 | loss 0.191 | nll_loss 0.002 | ppl 1.00 | wps 701 | ups 0 | wpb 1282.625 | bsz 15.625 | num_updates 48 | lr 8.08511e-06 | gnorm 4.629 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 256 | train_wall 87 | accuracy 0.959915
| epoch 003 | valid on 'valid' subset:  50%|█████████████████████████▌                         | 2/4 [00:01<00:01,  1.21it/s]/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
extra_meters:  defaultdict(<function validate.<locals>.<lambda> at 0x7f2666785268>, {'accuracy': <fairseq.meters.AverageMeter object at 0x7f266675e470>})
0.90332383665717
accuracy <fairseq.meters.AverageMeter object at 0x7f266675e470>
f1 <fairseq.meters.AverageMeter object at 0x7f2666774dd8>
| epoch 003 | valid on 'valid' subset | loss 0.210 | nll_loss 0.002 | ppl 1.00 | num_updates 48 | best_f1 0.903324 | accuracy 0.953125 | f1 0.903324
| saved checkpoint ../roberta.large.mnli/checkpoint3.pt (epoch 3 @ 48 updates) (writing took 80.71123695373535 seconds)
| epoch 004:   6%| | 1/16 [00:02<00:31,  2.10s/it, loss=0.070, nll_loss=0.001, ppl=1.00, wps=17, ups=0, wpb=1474.000, bsz=16.000, num_updates=49, lr=8.03191e-06, gnorm=1.027, clip=0.000, oom=0.000, loss_scale=4.000, wall=342, train_wall=89, accuracy=| epoch 004:  12%|▏| 2/16 [00:04<00:29,  2.12s/it, loss=0.074, nll_loss=0.001, ppl=1.00, wps=581, ups=0, wpb=1360.000, bsz=16.000, num_updates=50, lr=7.97872e-06, gnorm=1.069, clip=0.000, oom=0.000, loss_scale=4.000, wall=344, train_wall=91, accuracy| epoch 004:  19%|▏| 3/16 [00:05<00:26,  2.01s/it, loss=0.060, nll_loss=0.001, ppl=1.00, wps=607, ups=0, wpb=1279.000, bsz=16.000, num_updates=51, lr=7.92553e-06, gnorm=0.887, clip=0.000, oom=0.000, loss_scale=4.000, wall=346, train_wall=93, accuracy| epoch 004:  25%|▎| 4/16 [00:08<00:24,  2.06s/it, loss=0.075, nll_loss=0.001, ppl=1.00, wps=615, ups=0, wpb=1306.000, bsz=16.000, num_updates=52, lr=7.87234e-06, gnorm=2.882, clip=0.000, oom=0.000, loss_scale=4.000, wall=348, train_wall=95, accuracy| epoch 004:  31%|▎| 5/16 [00:09<00:21,  1.93s/it, loss=0.076, nll_loss=0.001, ppl=1.00, wps=648, ups=0, wpb=1294.800, bsz=16.000, num_updates=53, lr=7.81915e-06, gnorm=3.426, clip=0.000, oom=0.000, loss_scale=4.000, wall=350, train_wall=97, accuracy| epoch 004:  38%|▍| 6/16 [00:10<00:16,  1.64s/it, loss=0.077, nll_loss=0.001, ppl=1.00, wps=652, ups=0, wpb=1186.833, bsz=15.000, num_updates=54, lr=7.76596e-06, gnorm=3.030, clip=0.000, oom=0.000, loss_scale=4.000, wall=350, train_wall=98, accuracy| epoch 004:  44%|▍| 7/16 [00:12<00:16,  1.80s/it, loss=0.071, nll_loss=0.001, ppl=1.00, wps=664, ups=0, wpb=1240.857, bsz=15.143, num_updates=55, lr=7.71277e-06, gnorm=2.788, clip=0.000, oom=0.000, loss_scale=4.000, wall=353, train_wall=100, accurac| epoch 004:  50%|▌| 8/16 [00:14<00:14,  1.83s/it, loss=0.069, nll_loss=0.001, ppl=1.00, wps=647, ups=0, wpb=1214.625, bsz=15.250, num_updates=56, lr=7.65957e-06, gnorm=2.570, clip=0.000, oom=0.000, loss_scale=4.000, wall=355, train_wall=102, accurac| epoch 004:  56%|▌| 9/16 [00:16<00:13,  1.88s/it, loss=0.070, nll_loss=0.001, ppl=1.00, wps=659, ups=0, wpb=1244.111, bsz=15.333, num_updates=57, lr=7.60638e-06, gnorm=2.485, clip=0.000, oom=0.000, loss_scale=4.000, wall=357, train_wall=104, accurac| epoch 004:  62%|▋| 10/16 [00:18<00:11,  1.91s/it, loss=0.065, nll_loss=0.001, ppl=1.00, wps=660, ups=0, wpb=1250.000, bsz=15.400, num_updates=58, lr=7.55319e-06, gnorm=2.278, clip=0.000, oom=0.000, loss_scale=4.000, wall=358, train_wall=106, accura| epoch 004:  69%|▋| 11/16 [00:20<00:09,  1.96s/it, loss=0.068, nll_loss=0.001, ppl=1.00, wps=671, ups=0, wpb=1281.364, bsz=15.455, num_updates=59, lr=7.5e-06, gnorm=2.396, clip=0.000, oom=0.000, loss_scale=4.000, wall=361, train_wall=108, accuracy=1| epoch 004:  75%|▊| 12/16 [00:22<00:07,  1.96s/it, loss=0.064, nll_loss=0.001, ppl=1.00, wps=683, ups=0, wpb=1304.250, bsz=15.500, num_updates=60, lr=7.44681e-06, gnorm=2.260, clip=0.000, oom=0.000, loss_scale=4.000, wall=363, train_wall=110, accura| epoch 004:  81%|▊| 13/16 [00:24<00:05,  1.77s/it, loss=0.063, nll_loss=0.001, ppl=1.00, wps=688, ups=0, wpb=1282.231, bsz=15.538, num_updates=61, lr=7.39362e-06, gnorm=2.219, clip=0.000, oom=0.000, loss_scale=4.000, wall=364, train_wall=111, accura| epoch 004:  88%|▉| 14/16 [00:25<00:03,  1.77s/it, loss=0.064, nll_loss=0.001, ppl=1.00, wps=687, ups=0, wpb=1275.071, bsz=15.571, num_updates=62, lr=7.34043e-06, gnorm=2.178, clip=0.000, oom=0.000, loss_scale=4.000, wall=366, train_wall=113, accura| epoch 004:  94%|▉| 15/16 [00:27<00:01,  1.73s/it, loss=0.062, nll_loss=0.001, ppl=1.00, wps=695, ups=0, wpb=1279.200, bsz=15.600, num_updates=63, lr=7.28723e-06, gnorm=2.075, clip=0.000, oom=0.000, loss_scale=4.000, wall=367, train_wall=114, accura| epoch 004: 100%|█| 16/16 [00:29<00:00,  1.67s/it, loss=0.074, nll_loss=0.001, ppl=1.00, wps=705, ups=0, wpb=1282.625, bsz=15.625, num_updates=64, lr=7.23404e-06, gnorm=3.101, clip=0.000, oom=0.000, loss_scale=4.000, wall=369, train_wall=116, accura                                                                                                                                                                                                                                                          | epoch 004 | loss 0.074 | nll_loss 0.001 | ppl 1.00 | wps 704 | ups 0 | wpb 1282.625 | bsz 15.625 | num_updates 64 | lr 7.23404e-06 | gnorm 3.101 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 369 | train_wall 116 | accuracy 0.996006
extra_meters:  defaultdict(<function validate.<locals>.<lambda> at 0x7f26667b67b8>, {'accuracy': <fairseq.meters.AverageMeter object at 0x7f266677c748>})
0.8879163472186727
accuracy <fairseq.meters.AverageMeter object at 0x7f266677c748>
f1 <fairseq.meters.AverageMeter object at 0x7f2666774b70>
| epoch 004 | valid on 'valid' subset | loss 0.222 | nll_loss 0.002 | ppl 1.00 | num_updates 64 | best_f1 0.903324 | accuracy 0.9375 | f1 0.887916
| saved checkpoint ../roberta.large.mnli/checkpoint4.pt (epoch 4 @ 64 updates) (writing took 35.46296763420105 seconds)
| epoch 005:   6%| | 1/16 [00:02<00:31,  2.11s/it, loss=0.019, nll_loss=0.000, ppl=1.00, wps=39, ups=0, wpb=1556.000, bsz=16.000, num_updates=65, lr=7.18085e-06, gnorm=0.539, clip=0.000, oom=0.000, loss_scale=4.000, wall=409, train_wall=118, accuracy| epoch 005:  12%|▏| 2/16 [00:03<00:27,  1.97s/it, loss=0.030, nll_loss=0.000, ppl=1.00, wps=818, ups=0, wpb=1446.500, bsz=16.000, num_updates=66, lr=7.12766e-06, gnorm=0.701, clip=0.000, oom=0.000, loss_scale=4.000, wall=411, train_wall=120, accurac| epoch 005:  19%|▏| 3/16 [00:05<00:23,  1.79s/it, loss=0.041, nll_loss=0.001, ppl=1.00, wps=784, ups=0, wpb=1303.667, bsz=16.000, num_updates=67, lr=7.07447e-06, gnorm=2.350, clip=0.000, oom=0.000, loss_scale=4.000, wall=412, train_wall=121, accurac| epoch 005:  25%|▎| 4/16 [00:06<00:21,  1.78s/it, loss=0.035, nll_loss=0.000, ppl=1.00, wps=731, ups=0, wpb=1257.000, bsz=16.000, num_updates=68, lr=7.02128e-06, gnorm=1.867, clip=0.000, oom=0.000, loss_scale=4.000, wall=414, train_wall=123, accurac| epoch 005:  31%|▎| 5/16 [00:09<00:20,  1.90s/it, loss=0.032, nll_loss=0.000, ppl=1.00, wps=699, ups=0, wpb=1283.000, bsz=16.000, num_updates=69, lr=6.96809e-06, gnorm=1.594, clip=0.000, oom=0.000, loss_scale=4.000, wall=416, train_wall=125, accurac| epoch 005:  38%|▍| 6/16 [00:11<00:19,  1.94s/it, loss=0.034, nll_loss=0.000, ppl=1.00, wps=705, ups=0, wpb=1315.833, bsz=16.000, num_updates=70, lr=6.91489e-06, gnorm=1.419, clip=0.000, oom=0.000, loss_scale=4.000, wall=418, train_wall=127, accurac| epoch 005:  44%|▍| 7/16 [00:13<00:17,  1.95s/it, loss=0.033, nll_loss=0.000, ppl=1.00, wps=713, ups=0, wpb=1338.429, bsz=16.000, num_updates=71, lr=6.8617e-06, gnorm=1.294, clip=0.000, oom=0.000, loss_scale=4.000, wall=420, train_wall=129, accuracy| epoch 005:  50%|▌| 8/16 [00:14<00:15,  1.89s/it, loss=0.033, nll_loss=0.000, ppl=1.00, wps=708, ups=0, wpb=1318.875, bsz=16.000, num_updates=72, lr=6.80851e-06, gnorm=1.223, clip=0.000, oom=0.000, loss_scale=4.000, wall=422, train_wall=131, accurac| epoch 005:  56%|▌| 9/16 [00:16<00:13,  1.97s/it, loss=0.033, nll_loss=0.000, ppl=1.00, wps=690, ups=0, wpb=1310.778, bsz=16.000, num_updates=73, lr=6.75532e-06, gnorm=1.154, clip=0.000, oom=0.000, loss_scale=4.000, wall=424, train_wall=133, accurac| epoch 005:  62%|▋| 10/16 [00:19<00:12,  2.01s/it, loss=0.033, nll_loss=0.000, ppl=1.00, wps=698, ups=0, wpb=1339.200, bsz=16.000, num_updates=74, lr=6.70213e-06, gnorm=1.277, clip=0.000, oom=0.000, loss_scale=4.000, wall=426, train_wall=135, accura| epoch 005:  69%|▋| 11/16 [00:21<00:10,  2.04s/it, loss=0.032, nll_loss=0.000, ppl=1.00, wps=703, ups=0, wpb=1359.727, bsz=16.000, num_updates=75, lr=6.64894e-06, gnorm=1.222, clip=0.000, oom=0.000, loss_scale=4.000, wall=428, train_wall=137, accura| epoch 005:  75%|▊| 12/16 [00:22<00:06,  1.71s/it, loss=0.031, nll_loss=0.000, ppl=1.00, wps=702, ups=0, wpb=1300.333, bsz=15.500, num_updates=76, lr=6.59574e-06, gnorm=1.155, clip=0.000, oom=0.000, loss_scale=4.000, wall=429, train_wall=138, accura| epoch 005:  81%|▊| 13/16 [00:24<00:05,  1.77s/it, loss=0.031, nll_loss=0.000, ppl=1.00, wps=688, ups=0, wpb=1279.615, bsz=15.538, num_updates=77, lr=6.54255e-06, gnorm=1.183, clip=0.000, oom=0.000, loss_scale=4.000, wall=431, train_wall=140, accura| epoch 005:  88%|▉| 14/16 [00:25<00:03,  1.70s/it, loss=0.033, nll_loss=0.000, ppl=1.00, wps=700, ups=0, wpb=1283.500, bsz=15.571, num_updates=78, lr=6.48936e-06, gnorm=1.472, clip=0.000, oom=0.000, loss_scale=4.000, wall=433, train_wall=141, accura| epoch 005:  94%|▉| 15/16 [00:27<00:01,  1.78s/it, loss=0.031, nll_loss=0.000, ppl=1.00, wps=697, ups=0, wpb=1284.800, bsz=15.600, num_updates=79, lr=6.43617e-06, gnorm=1.387, clip=0.000, oom=0.000, loss_scale=4.000, wall=435, train_wall=143, accura| epoch 005: 100%|█| 16/16 [00:29<00:00,  1.73s/it, loss=0.031, nll_loss=0.000, ppl=1.00, wps=702, ups=0, wpb=1282.625, bsz=15.625, num_updates=80, lr=6.38298e-06, gnorm=1.316, clip=0.000, oom=0.000, loss_scale=4.000, wall=436, train_wall=145, accura                                                                                                                                                                                                                                                          | epoch 005 | loss 0.031 | nll_loss 0.000 | ppl 1.00 | wps 700 | ups 0 | wpb 1282.625 | bsz 15.625 | num_updates 80 | lr 6.38298e-06 | gnorm 1.316 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 436 | train_wall 145 | accuracy 1
extra_meters:  defaultdict(<function validate.<locals>.<lambda> at 0x7f26667418c8>, {'accuracy': <fairseq.meters.AverageMeter object at 0x7f266677cba8>})
0.8879163472186727
accuracy <fairseq.meters.AverageMeter object at 0x7f266677cba8>
f1 <fairseq.meters.AverageMeter object at 0x7f26665ac0f0>
| epoch 005 | valid on 'valid' subset | loss 0.280 | nll_loss 0.003 | ppl 1.00 | num_updates 80 | best_f1 0.903324 | accuracy 0.9375 | f1 0.887916
| saved checkpoint ../roberta.large.mnli/checkpoint5.pt (epoch 5 @ 80 updates) (writing took 31.233640909194946 seconds)
| epoch 006:   6%| | 1/16 [00:02<00:32,  2.17s/it, loss=0.026, nll_loss=0.000, ppl=1.00, wps=41, ups=0, wpb=1480.000, bsz=16.000, num_updates=81, lr=6.32979e-06, gnorm=0.353, clip=0.000, oom=0.000, loss_scale=4.000, wall=472, train_wall=147, accuracy| epoch 006:  12%|▏| 2/16 [00:04<00:29,  2.10s/it, loss=0.022, nll_loss=0.000, ppl=1.00, wps=754, ups=0, wpb=1477.000, bsz=16.000, num_updates=82, lr=6.2766e-06, gnorm=0.321, clip=0.000, oom=0.000, loss_scale=4.000, wall=474, train_wall=149, accuracy| epoch 006:  19%|▏| 3/16 [00:05<00:25,  1.96s/it, loss=0.021, nll_loss=0.000, ppl=1.00, wps=760, ups=0, wpb=1401.333, bsz=16.000, num_updates=83, lr=6.2234e-06, gnorm=0.356, clip=0.000, oom=0.000, loss_scale=4.000, wall=476, train_wall=151, accuracy| epoch 006:  25%|▎| 4/16 [00:07<00:24,  2.03s/it, loss=0.019, nll_loss=0.000, ppl=1.00, wps=710, ups=0, wpb=1397.750, bsz=16.000, num_updates=84, lr=6.17021e-06, gnorm=0.318, clip=0.000, oom=0.000, loss_scale=4.000, wall=478, train_wall=153, accurac| epoch 006:  31%|▎| 5/16 [00:09<00:22,  2.01s/it, loss=0.017, nll_loss=0.000, ppl=1.00, wps=732, ups=0, wpb=1429.400, bsz=16.000, num_updates=85, lr=6.11702e-06, gnorm=0.302, clip=0.000, oom=0.000, loss_scale=4.000, wall=480, train_wall=155, accurac| epoch 006:  38%|▍| 6/16 [00:12<00:20,  2.03s/it, loss=0.018, nll_loss=0.000, ppl=1.00, wps=738, ups=0, wpb=1457.000, bsz=16.000, num_updates=86, lr=6.06383e-06, gnorm=0.299, clip=0.000, oom=0.000, loss_scale=4.000, wall=482, train_wall=157, accurac| epoch 006:  44%|▍| 7/16 [00:14<00:18,  2.07s/it, loss=0.017, nll_loss=0.000, ppl=1.00, wps=710, ups=0, wpb=1426.857, bsz=16.000, num_updates=87, lr=6.01064e-06, gnorm=0.287, clip=0.000, oom=0.000, loss_scale=4.000, wall=484, train_wall=159, accurac| epoch 006:  50%|▌| 8/16 [00:15<00:15,  1.94s/it, loss=0.017, nll_loss=0.000, ppl=1.00, wps=723, ups=0, wpb=1415.625, bsz=16.000, num_updates=88, lr=5.95745e-06, gnorm=0.292, clip=0.000, oom=0.000, loss_scale=4.000, wall=486, train_wall=161, accurac| epoch 006:  56%|▌| 9/16 [00:17<00:13,  1.89s/it, loss=0.017, nll_loss=0.000, ppl=1.00, wps=717, ups=0, wpb=1389.667, bsz=16.000, num_updates=89, lr=5.90426e-06, gnorm=0.287, clip=0.000, oom=0.000, loss_scale=4.000, wall=488, train_wall=162, accurac| epoch 006:  62%|▋| 10/16 [00:19<00:11,  1.85s/it, loss=0.017, nll_loss=0.000, ppl=1.00, wps=709, ups=0, wpb=1362.400, bsz=16.000, num_updates=90, lr=5.85106e-06, gnorm=0.277, clip=0.000, oom=0.000, loss_scale=4.000, wall=489, train_wall=164, accura| epoch 006:  69%|▋| 11/16 [00:21<00:09,  1.92s/it, loss=0.017, nll_loss=0.000, ppl=1.00, wps=713, ups=0, wpb=1380.818, bsz=16.000, num_updates=91, lr=5.79787e-06, gnorm=0.406, clip=0.000, oom=0.000, loss_scale=4.000, wall=491, train_wall=166, accura| epoch 006:  75%|▊| 12/16 [00:22<00:07,  1.81s/it, loss=0.018, nll_loss=0.000, ppl=1.00, wps=724, ups=0, wpb=1376.917, bsz=16.000, num_updates=92, lr=5.74468e-06, gnorm=0.406, clip=0.000, oom=0.000, loss_scale=4.000, wall=493, train_wall=168, accura| epoch 006:  81%|▊| 13/16 [00:24<00:05,  1.67s/it, loss=0.018, nll_loss=0.000, ppl=1.00, wps=725, ups=0, wpb=1349.308, bsz=16.000, num_updates=93, lr=5.69149e-06, gnorm=0.399, clip=0.000, oom=0.000, loss_scale=4.000, wall=494, train_wall=169, accura| epoch 006:  88%|▉| 14/16 [00:26<00:03,  1.76s/it, loss=0.017, nll_loss=0.000, ppl=1.00, wps=720, ups=0, wpb=1346.000, bsz=16.000, num_updates=94, lr=5.6383e-06, gnorm=0.391, clip=0.000, oom=0.000, loss_scale=4.000, wall=496, train_wall=171, accurac| epoch 006:  94%|▉| 15/16 [00:28<00:01,  1.80s/it, loss=0.017, nll_loss=0.000, ppl=1.00, wps=707, ups=0, wpb=1325.000, bsz=16.000, num_updates=95, lr=5.58511e-06, gnorm=0.377, clip=0.000, oom=0.000, loss_scale=4.000, wall=498, train_wall=173, accura| epoch 006: 100%|█| 16/16 [00:29<00:00,  1.54s/it, loss=0.017, nll_loss=0.000, ppl=1.00, wps=707, ups=0, wpb=1282.625, bsz=15.625, num_updates=96, lr=5.53191e-06, gnorm=0.371, clip=0.000, oom=0.000, loss_scale=4.000, wall=499, train_wall=174, accura                                                                                                                                                                                                                                                          | epoch 006 | loss 0.017 | nll_loss 0.000 | ppl 1.00 | wps 705 | ups 0 | wpb 1282.625 | bsz 15.625 | num_updates 96 | lr 5.53191e-06 | gnorm 0.371 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 499 | train_wall 174 | accuracy 1
extra_meters:  defaultdict(<function validate.<locals>.<lambda> at 0x7f26667852f0>, {'accuracy': <fairseq.meters.AverageMeter object at 0x7f266675e6a0>})
0.9018648018648019
accuracy <fairseq.meters.AverageMeter object at 0x7f266675e6a0>
f1 <fairseq.meters.AverageMeter object at 0x7f2666777a58>
| epoch 006 | valid on 'valid' subset | loss 0.256 | nll_loss 0.003 | ppl 1.00 | num_updates 96 | best_f1 0.903324 | accuracy 0.953125 | f1 0.901865
| saved checkpoint ../roberta.large.mnli/checkpoint6.pt (epoch 6 @ 96 updates) (writing took 30.87694001197815 seconds)
| epoch 007:   6%| | 1/16 [00:01<00:22,  1.48s/it, loss=0.013, nll_loss=0.000, ppl=1.00, wps=29, ups=0, wpb=1018.000, bsz=16.000, num_updates=97, lr=5.47872e-06, gnorm=0.201, clip=0.000, oom=0.000, loss_scale=4.000, wall=534, train_wall=175, accuracy| epoch 007:  12%|▏| 2/16 [00:03<00:22,  1.62s/it, loss=0.011, nll_loss=0.000, ppl=1.00, wps=794, ups=0, wpb=1287.000, bsz=16.000, num_updates=98, lr=5.42553e-06, gnorm=0.179, clip=0.000, oom=0.000, loss_scale=4.000, wall=536, train_wall=177, accurac| epoch 007:  19%|▏| 3/16 [00:05<00:23,  1.80s/it, loss=0.011, nll_loss=0.000, ppl=1.00, wps=707, ups=0, wpb=1320.333, bsz=16.000, num_updates=99, lr=5.37234e-06, gnorm=0.286, clip=0.000, oom=0.000, loss_scale=4.000, wall=538, train_wall=179, accurac| epoch 007:  25%|▎| 4/16 [00:07<00:21,  1.79s/it, loss=0.012, nll_loss=0.000, ppl=1.00, wps=697, ups=0, wpb=1285.750, bsz=16.000, num_updates=100, lr=5.31915e-06, gnorm=0.268, clip=0.000, oom=0.000, loss_scale=4.000, wall=540, train_wall=181, accura| epoch 007:  31%|▎| 5/16 [00:09<00:19,  1.74s/it, loss=0.012, nll_loss=0.000, ppl=1.00, wps=724, ups=0, wpb=1296.000, bsz=16.000, num_updates=101, lr=5.26596e-06, gnorm=0.251, clip=0.000, oom=0.000, loss_scale=4.000, wall=542, train_wall=183, accura| epoch 007:  38%|▍| 6/16 [00:11<00:18,  1.84s/it, loss=0.012, nll_loss=0.000, ppl=1.00, wps=730, ups=0, wpb=1340.833, bsz=16.000, num_updates=102, lr=5.21277e-06, gnorm=0.235, clip=0.000, oom=0.000, loss_scale=4.000, wall=544, train_wall=185, accura| epoch 007:  44%|▍| 7/16 [00:12<00:16,  1.81s/it, loss=0.011, nll_loss=0.000, ppl=1.00, wps=716, ups=0, wpb=1308.857, bsz=16.000, num_updates=103, lr=5.15957e-06, gnorm=0.223, clip=0.000, oom=0.000, loss_scale=4.000, wall=546, train_wall=187, accura| epoch 007:  50%|▌| 8/16 [00:14<00:15,  1.90s/it, loss=0.011, nll_loss=0.000, ppl=1.00, wps=723, ups=0, wpb=1344.625, bsz=16.000, num_updates=104, lr=5.10638e-06, gnorm=0.215, clip=0.000, oom=0.000, loss_scale=4.000, wall=548, train_wall=189, accura| epoch 007:  56%|▌| 9/16 [00:15<00:11,  1.61s/it, loss=0.011, nll_loss=0.000, ppl=1.00, wps=720, ups=0, wpb=1267.111, bsz=15.333, num_updates=105, lr=5.05319e-06, gnorm=0.210, clip=0.000, oom=0.000, loss_scale=4.000, wall=549, train_wall=190, accura| epoch 007:  62%|▋| 10/16 [00:17<00:10,  1.73s/it, loss=0.012, nll_loss=0.000, ppl=1.00, wps=722, ups=0, wpb=1288.400, bsz=15.400, num_updates=106, lr=5e-06, gnorm=0.205, clip=0.000, oom=0.000, loss_scale=4.000, wall=551, train_wall=192, accuracy=1]| epoch 007:  69%|▋| 11/16 [00:19<00:08,  1.70s/it, loss=0.011, nll_loss=0.000, ppl=1.00, wps=726, ups=0, wpb=1284.909, bsz=15.455, num_updates=107, lr=4.94681e-06, gnorm=0.199, clip=0.000, oom=0.000, loss_scale=4.000, wall=552, train_wall=193, accur| epoch 007:  75%|▊| 12/16 [00:21<00:07,  1.78s/it, loss=0.011, nll_loss=0.000, ppl=1.00, wps=728, ups=0, wpb=1300.667, bsz=15.500, num_updates=108, lr=4.89362e-06, gnorm=0.196, clip=0.000, oom=0.000, loss_scale=4.000, wall=554, train_wall=195, accur| epoch 007:  81%|▊| 13/16 [00:23<00:05,  1.83s/it, loss=0.011, nll_loss=0.000, ppl=1.00, wps=723, ups=0, wpb=1300.846, bsz=15.538, num_updates=109, lr=4.84043e-06, gnorm=0.199, clip=0.000, oom=0.000, loss_scale=4.000, wall=556, train_wall=197, accur| epoch 007:  88%|▉| 14/16 [00:25<00:03,  1.93s/it, loss=0.011, nll_loss=0.000, ppl=1.00, wps=710, ups=0, wpb=1296.929, bsz=15.571, num_updates=110, lr=4.78723e-06, gnorm=0.195, clip=0.000, oom=0.000, loss_scale=4.000, wall=558, train_wall=199, accur| epoch 007:  94%|▉| 15/16 [00:27<00:01,  1.92s/it, loss=0.011, nll_loss=0.000, ppl=1.00, wps=697, ups=0, wpb=1279.200, bsz=15.600, num_updates=111, lr=4.73404e-06, gnorm=0.190, clip=0.000, oom=0.000, loss_scale=4.000, wall=560, train_wall=201, accur| epoch 007: 100%|█| 16/16 [00:29<00:00,  1.81s/it, loss=0.011, nll_loss=0.000, ppl=1.00, wps=707, ups=0, wpb=1282.625, bsz=15.625, num_updates=112, lr=4.68085e-06, gnorm=0.196, clip=0.000, oom=0.000, loss_scale=4.000, wall=562, train_wall=203, accur                                                                                                                                                                                                                                                          | epoch 007 | loss 0.011 | nll_loss 0.000 | ppl 1.00 | wps 705 | ups 0 | wpb 1282.625 | bsz 15.625 | num_updates 112 | lr 4.68085e-06 | gnorm 0.196 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 562 | train_wall 203 | accuracy 1
extra_meters:  defaultdict(<function validate.<locals>.<lambda> at 0x7f2666741950>, {'accuracy': <fairseq.meters.AverageMeter object at 0x7f26b9d83128>})
0.9018648018648019
accuracy <fairseq.meters.AverageMeter object at 0x7f26b9d83128>
f1 <fairseq.meters.AverageMeter object at 0x7f266677cdd8>
| epoch 007 | valid on 'valid' subset | loss 0.254 | nll_loss 0.003 | ppl 1.00 | num_updates 112 | best_f1 0.903324 | accuracy 0.953125 | f1 0.901865
| saved checkpoint ../roberta.large.mnli/checkpoint7.pt (epoch 7 @ 112 updates) (writing took 30.6102237701416 seconds)
| epoch 008:   6%| | 1/16 [00:01<00:28,  1.91s/it, loss=0.008, nll_loss=0.000, ppl=1.00, wps=32, ups=0, wpb=1117.000, bsz=16.000, num_updates=113, lr=4.62766e-06, gnorm=0.117, clip=0.000, oom=0.000, loss_scale=4.000, wall=597, train_wall=205, accurac| epoch 008:  12%|▏| 2/16 [00:03<00:27,  1.95s/it, loss=0.008, nll_loss=0.000, ppl=1.00, wps=759, ups=0, wpb=1341.000, bsz=16.000, num_updates=114, lr=4.57447e-06, gnorm=0.132, clip=0.000, oom=0.000, loss_scale=4.000, wall=599, train_wall=207, accura| epoch 008:  19%|▏| 3/16 [00:05<00:24,  1.89s/it, loss=0.009, nll_loss=0.000, ppl=1.00, wps=722, ups=0, wpb=1288.000, bsz=16.000, num_updates=115, lr=4.52128e-06, gnorm=0.136, clip=0.000, oom=0.000, loss_scale=4.000, wall=601, train_wall=208, accura| epoch 008:  25%|▎| 4/16 [00:07<00:22,  1.91s/it, loss=0.009, nll_loss=0.000, ppl=1.00, wps=702, ups=0, wpb=1291.750, bsz=16.000, num_updates=116, lr=4.46809e-06, gnorm=0.132, clip=0.000, oom=0.000, loss_scale=4.000, wall=603, train_wall=210, accura| epoch 008:  31%|▎| 5/16 [00:09<00:21,  2.00s/it, loss=0.008, nll_loss=0.000, ppl=1.00, wps=683, ups=0, wpb=1310.800, bsz=16.000, num_updates=117, lr=4.41489e-06, gnorm=0.131, clip=0.000, oom=0.000, loss_scale=4.000, wall=605, train_wall=213, accura| epoch 008:  38%|▍| 6/16 [00:11<00:20,  2.03s/it, loss=0.009, nll_loss=0.000, ppl=1.00, wps=699, ups=0, wpb=1358.167, bsz=16.000, num_updates=118, lr=4.3617e-06, gnorm=0.132, clip=0.000, oom=0.000, loss_scale=4.000, wall=607, train_wall=215, accurac| epoch 008:  44%|▍| 7/16 [00:13<00:16,  1.89s/it, loss=0.010, nll_loss=0.000, ppl=1.00, wps=720, ups=0, wpb=1354.714, bsz=16.000, num_updates=119, lr=4.30851e-06, gnorm=0.147, clip=0.000, oom=0.000, loss_scale=4.000, wall=609, train_wall=216, accura| epoch 008:  50%|▌| 8/16 [00:15<00:15,  1.89s/it, loss=0.010, nll_loss=0.000, ppl=1.00, wps=695, ups=0, wpb=1314.250, bsz=16.000, num_updates=120, lr=4.25532e-06, gnorm=0.144, clip=0.000, oom=0.000, loss_scale=4.000, wall=611, train_wall=218, accura| epoch 008:  56%|▌| 9/16 [00:17<00:12,  1.81s/it, loss=0.009, nll_loss=0.000, ppl=1.00, wps=703, ups=0, wpb=1307.111, bsz=16.000, num_updates=121, lr=4.20213e-06, gnorm=0.141, clip=0.000, oom=0.000, loss_scale=4.000, wall=612, train_wall=220, accura| epoch 008:  62%|▋| 10/16 [00:19<00:11,  1.91s/it, loss=0.009, nll_loss=0.000, ppl=1.00, wps=687, ups=0, wpb=1301.000, bsz=16.000, num_updates=122, lr=4.14894e-06, gnorm=0.139, clip=0.000, oom=0.000, loss_scale=4.000, wall=614, train_wall=222, accur| epoch 008:  69%|▋| 11/16 [00:20<00:08,  1.63s/it, loss=0.009, nll_loss=0.000, ppl=1.00, wps=687, ups=0, wpb=1241.545, bsz=15.455, num_updates=123, lr=4.09574e-06, gnorm=0.141, clip=0.000, oom=0.000, loss_scale=4.000, wall=615, train_wall=223, accur| epoch 008:  75%|▊| 12/16 [00:22<00:06,  1.75s/it, loss=0.009, nll_loss=0.000, ppl=1.00, wps=691, ups=0, wpb=1261.417, bsz=15.500, num_updates=124, lr=4.04255e-06, gnorm=0.141, clip=0.000, oom=0.000, loss_scale=4.000, wall=617, train_wall=225, accur| epoch 008:  81%|▊| 13/16 [00:23<00:04,  1.63s/it, loss=0.009, nll_loss=0.000, ppl=1.00, wps=694, ups=0, wpb=1242.692, bsz=15.538, num_updates=125, lr=3.98936e-06, gnorm=0.140, clip=0.000, oom=0.000, loss_scale=4.000, wall=619, train_wall=226, accur| epoch 008:  88%|▉| 14/16 [00:25<00:03,  1.73s/it, loss=0.009, nll_loss=0.000, ppl=1.00, wps=699, ups=0, wpb=1259.214, bsz=15.571, num_updates=126, lr=3.93617e-06, gnorm=0.140, clip=0.000, oom=0.000, loss_scale=4.000, wall=621, train_wall=228, accur| epoch 008:  94%|▉| 15/16 [00:27<00:01,  1.80s/it, loss=0.009, nll_loss=0.000, ppl=1.00, wps=707, ups=0, wpb=1279.000, bsz=15.600, num_updates=127, lr=3.88298e-06, gnorm=0.137, clip=0.000, oom=0.000, loss_scale=4.000, wall=623, train_wall=230, accur| epoch 008: 100%|█| 16/16 [00:29<00:00,  1.75s/it, loss=0.009, nll_loss=0.000, ppl=1.00, wps=714, ups=0, wpb=1282.625, bsz=15.625, num_updates=128, lr=3.82979e-06, gnorm=0.135, clip=0.000, oom=0.000, loss_scale=8.000, wall=624, train_wall=232, accur                                                                                                                                                                                                                                                          | epoch 008 | loss 0.009 | nll_loss 0.000 | ppl 1.00 | wps 712 | ups 0 | wpb 1282.625 | bsz 15.625 | num_updates 128 | lr 3.82979e-06 | gnorm 0.135 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 624 | train_wall 232 | accuracy 1
extra_meters:  defaultdict(<function validate.<locals>.<lambda> at 0x7f2666785378>, {'accuracy': <fairseq.meters.AverageMeter object at 0x7f26b9d83e48>})
0.9018648018648019
accuracy <fairseq.meters.AverageMeter object at 0x7f26b9d83e48>
f1 <fairseq.meters.AverageMeter object at 0x7f26665c5fd0>
| epoch 008 | valid on 'valid' subset | loss 0.260 | nll_loss 0.003 | ppl 1.00 | num_updates 128 | best_f1 0.903324 | accuracy 0.953125 | f1 0.901865
| saved checkpoint ../roberta.large.mnli/checkpoint8.pt (epoch 8 @ 128 updates) (writing took 30.88396692276001 seconds)
| epoch 009:   6%| | 1/16 [00:02<00:31,  2.07s/it, loss=0.009, nll_loss=0.000, ppl=1.00, wps=41, ups=0, wpb=1474.000, bsz=16.000, num_updates=129, lr=3.7766e-06, gnorm=0.144, clip=0.000, oom=0.000, loss_scale=8.000, wall=660, train_wall=234, accuracy| epoch 009:  12%|▏| 2/16 [00:03<00:26,  1.92s/it, loss=0.012, nll_loss=0.000, ppl=1.00, wps=850, ups=0, wpb=1404.000, bsz=16.000, num_updates=130, lr=3.7234e-06, gnorm=0.233, clip=0.000, oom=0.000, loss_scale=8.000, wall=662, train_wall=235, accurac| epoch 009:  19%|▏| 3/16 [00:05<00:23,  1.84s/it, loss=0.011, nll_loss=0.000, ppl=1.00, wps=833, ups=0, wpb=1381.667, bsz=16.000, num_updates=131, lr=3.67021e-06, gnorm=0.197, clip=0.000, oom=0.000, loss_scale=8.000, wall=663, train_wall=237, accura| epoch 009:  25%|▎| 4/16 [00:07<00:21,  1.81s/it, loss=0.010, nll_loss=0.000, ppl=1.00, wps=763, ups=0, wpb=1315.500, bsz=16.000, num_updates=132, lr=3.61702e-06, gnorm=0.173, clip=0.000, oom=0.000, loss_scale=8.000, wall=665, train_wall=239, accura| epoch 009:  31%|▎| 5/16 [00:08<00:18,  1.68s/it, loss=0.010, nll_loss=0.000, ppl=1.00, wps=759, ups=0, wpb=1256.000, bsz=16.000, num_updates=133, lr=3.56383e-06, gnorm=0.164, clip=0.000, oom=0.000, loss_scale=8.000, wall=666, train_wall=240, accura| epoch 009:  38%|▍| 6/16 [00:10<00:17,  1.79s/it, loss=0.010, nll_loss=0.000, ppl=1.00, wps=751, ups=0, wpb=1293.333, bsz=16.000, num_updates=134, lr=3.51064e-06, gnorm=0.160, clip=0.000, oom=0.000, loss_scale=8.000, wall=668, train_wall=242, accura| epoch 009:  44%|▍| 7/16 [00:12<00:15,  1.74s/it, loss=0.009, nll_loss=0.000, ppl=1.00, wps=752, ups=0, wpb=1287.143, bsz=16.000, num_updates=135, lr=3.45745e-06, gnorm=0.170, clip=0.000, oom=0.000, loss_scale=8.000, wall=670, train_wall=244, accura| epoch 009:  50%|▌| 8/16 [00:14<00:15,  1.89s/it, loss=0.009, nll_loss=0.000, ppl=1.00, wps=729, ups=0, wpb=1299.625, bsz=16.000, num_updates=136, lr=3.40426e-06, gnorm=0.166, clip=0.000, oom=0.000, loss_scale=8.000, wall=672, train_wall=246, accura| epoch 009:  56%|▌| 9/16 [00:16<00:13,  1.94s/it, loss=0.009, nll_loss=0.000, ppl=1.00, wps=733, ups=0, wpb=1329.111, bsz=16.000, num_updates=137, lr=3.35106e-06, gnorm=0.158, clip=0.000, oom=0.000, loss_scale=8.000, wall=674, train_wall=248, accura| epoch 009:  62%|▋| 10/16 [00:18<00:11,  1.95s/it, loss=0.008, nll_loss=0.000, ppl=1.00, wps=740, ups=0, wpb=1351.800, bsz=16.000, num_updates=138, lr=3.29787e-06, gnorm=0.151, clip=0.000, oom=0.000, loss_scale=8.000, wall=676, train_wall=250, accur| epoch 009:  69%|▋| 11/16 [00:19<00:08,  1.65s/it, loss=0.008, nll_loss=0.000, ppl=1.00, wps=737, ups=0, wpb=1287.727, bsz=15.455, num_updates=139, lr=3.24468e-06, gnorm=0.148, clip=0.000, oom=0.000, loss_scale=8.000, wall=677, train_wall=251, accur| epoch 009:  75%|▊| 12/16 [00:21<00:07,  1.81s/it, loss=0.008, nll_loss=0.000, ppl=1.00, wps=718, ups=0, wpb=1284.250, bsz=15.500, num_updates=140, lr=3.19149e-06, gnorm=0.144, clip=0.000, oom=0.000, loss_scale=8.000, wall=679, train_wall=253, accur| epoch 009:  81%|▊| 13/16 [00:23<00:05,  1.90s/it, loss=0.008, nll_loss=0.000, ppl=1.00, wps=722, ups=0, wpb=1308.154, bsz=15.538, num_updates=141, lr=3.1383e-06, gnorm=0.145, clip=0.000, oom=0.000, loss_scale=8.000, wall=681, train_wall=255, accura| epoch 009:  88%|▉| 14/16 [00:25<00:03,  1.90s/it, loss=0.008, nll_loss=0.000, ppl=1.00, wps=707, ups=0, wpb=1288.357, bsz=15.571, num_updates=142, lr=3.08511e-06, gnorm=0.142, clip=0.000, oom=0.000, loss_scale=8.000, wall=683, train_wall=257, accur| epoch 009:  94%|▉| 15/16 [00:27<00:01,  1.86s/it, loss=0.008, nll_loss=0.000, ppl=1.00, wps=704, ups=0, wpb=1281.267, bsz=15.600, num_updates=143, lr=3.03191e-06, gnorm=0.140, clip=0.000, oom=0.000, loss_scale=8.000, wall=685, train_wall=259, accur| epoch 009: 100%|█| 16/16 [00:29<00:00,  1.89s/it, loss=0.008, nll_loss=0.000, ppl=1.00, wps=702, ups=0, wpb=1282.625, bsz=15.625, num_updates=144, lr=2.97872e-06, gnorm=0.136, clip=0.000, oom=0.000, loss_scale=8.000, wall=687, train_wall=261, accur                                                                                                                                                                                                                                                          | epoch 009 | loss 0.008 | nll_loss 0.000 | ppl 1.00 | wps 700 | ups 0 | wpb 1282.625 | bsz 15.625 | num_updates 144 | lr 2.97872e-06 | gnorm 0.136 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 687 | train_wall 261 | accuracy 1
extra_meters:  defaultdict(<function validate.<locals>.<lambda> at 0x7f270d0e81e0>, {'accuracy': <fairseq.meters.AverageMeter object at 0x7f266677c7f0>})
0.9309764309764309
accuracy <fairseq.meters.AverageMeter object at 0x7f266677c7f0>
f1 <fairseq.meters.AverageMeter object at 0x7f2666796278>
| epoch 009 | valid on 'valid' subset | loss 0.269 | nll_loss 0.003 | ppl 1.00 | num_updates 144 | best_f1 0.930976 | accuracy 0.96875 | f1 0.930976
| saved checkpoint ../roberta.large.mnli/checkpoint9.pt (epoch 9 @ 144 updates) (writing took 74.26344275474548 seconds)
| epoch 010:   6%| | 1/16 [00:02<00:31,  2.12s/it, loss=0.008, nll_loss=0.000, ppl=1.00, wps=19, ups=0, wpb=1480.000, bsz=16.000, num_updates=145, lr=2.92553e-06, gnorm=0.104, clip=0.000, oom=0.000, loss_scale=8.000, wall=766, train_wall=263, accurac| epoch 010:  12%|▏| 2/16 [00:03<00:27,  1.95s/it, loss=0.010, nll_loss=0.000, ppl=1.00, wps=859, ups=0, wpb=1407.000, bsz=16.000, num_updates=146, lr=2.87234e-06, gnorm=0.154, clip=0.000, oom=0.000, loss_scale=8.000, wall=768, train_wall=264, accura| epoch 010:  19%|▏| 3/16 [00:05<00:23,  1.77s/it, loss=0.009, nll_loss=0.000, ppl=1.00, wps=808, ups=0, wpb=1277.333, bsz=16.000, num_updates=147, lr=2.81915e-06, gnorm=0.138, clip=0.000, oom=0.000, loss_scale=8.000, wall=769, train_wall=266, accura| epoch 010:  25%|▎| 4/16 [00:06<00:20,  1.73s/it, loss=0.009, nll_loss=0.000, ppl=1.00, wps=794, ups=0, wpb=1270.500, bsz=16.000, num_updates=148, lr=2.76596e-06, gnorm=0.129, clip=0.000, oom=0.000, loss_scale=8.000, wall=771, train_wall=267, accura| epoch 010:  31%|▎| 5/16 [00:07<00:16,  1.49s/it, loss=0.008, nll_loss=0.000, ppl=1.00, wps=775, ups=0, wpb=1145.800, bsz=14.800, num_updates=149, lr=2.71277e-06, gnorm=0.123, clip=0.000, oom=0.000, loss_scale=8.000, wall=772, train_wall=268, accura| epoch 010:  38%|▍| 6/16 [00:09<00:16,  1.62s/it, loss=0.008, nll_loss=0.000, ppl=1.00, wps=715, ups=0, wpb=1126.667, bsz=15.000, num_updates=150, lr=2.65957e-06, gnorm=0.118, clip=0.000, oom=0.000, loss_scale=8.000, wall=774, train_wall=270, accura| epoch 010:  44%|▍| 7/16 [00:11<00:15,  1.72s/it, loss=0.008, nll_loss=0.000, ppl=1.00, wps=723, ups=0, wpb=1176.286, bsz=15.143, num_updates=151, lr=2.60638e-06, gnorm=0.116, clip=0.000, oom=0.000, loss_scale=8.000, wall=776, train_wall=272, accura| epoch 010:  50%|▌| 8/16 [00:13<00:13,  1.69s/it, loss=0.008, nll_loss=0.000, ppl=1.00, wps=738, ups=0, wpb=1196.375, bsz=15.250, num_updates=152, lr=2.55319e-06, gnorm=0.118, clip=0.000, oom=0.000, loss_scale=8.000, wall=777, train_wall=274, accura| epoch 010:  56%|▌| 9/16 [00:15<00:12,  1.84s/it, loss=0.008, nll_loss=0.000, ppl=1.00, wps=720, ups=0, wpb=1217.556, bsz=15.333, num_updates=153, lr=2.5e-06, gnorm=0.114, clip=0.000, oom=0.000, loss_scale=8.000, wall=779, train_wall=276, accuracy=1| epoch 010:  62%|▋| 10/16 [00:17<00:10,  1.81s/it, loss=0.008, nll_loss=0.000, ppl=1.00, wps=715, ups=0, wpb=1214.000, bsz=15.400, num_updates=154, lr=2.44681e-06, gnorm=0.114, clip=0.000, oom=0.000, loss_scale=8.000, wall=781, train_wall=278, accur| epoch 010:  69%|▋| 11/16 [00:18<00:09,  1.85s/it, loss=0.007, nll_loss=0.000, ppl=1.00, wps=725, ups=0, wpb=1245.091, bsz=15.455, num_updates=155, lr=2.39362e-06, gnorm=0.113, clip=0.000, oom=0.000, loss_scale=8.000, wall=783, train_wall=279, accur| epoch 010:  75%|▊| 12/16 [00:20<00:07,  1.82s/it, loss=0.007, nll_loss=0.000, ppl=1.00, wps=717, ups=0, wpb=1234.417, bsz=15.500, num_updates=156, lr=2.34043e-06, gnorm=0.110, clip=0.000, oom=0.000, loss_scale=8.000, wall=785, train_wall=281, accur| epoch 010:  81%|▊| 13/16 [00:22<00:05,  1.92s/it, loss=0.007, nll_loss=0.000, ppl=1.00, wps=703, ups=0, wpb=1235.308, bsz=15.538, num_updates=157, lr=2.28723e-06, gnorm=0.108, clip=0.000, oom=0.000, loss_scale=8.000, wall=787, train_wall=283, accur| epoch 010:  88%|▉| 14/16 [00:24<00:03,  1.96s/it, loss=0.007, nll_loss=0.000, ppl=1.00, wps=708, ups=0, wpb=1258.857, bsz=15.571, num_updates=158, lr=2.23404e-06, gnorm=0.106, clip=0.000, oom=0.000, loss_scale=8.000, wall=789, train_wall=285, accur| epoch 010:  94%|▉| 15/16 [00:27<00:01,  2.00s/it, loss=0.007, nll_loss=0.000, ppl=1.00, wps=713, ups=0, wpb=1281.267, bsz=15.600, num_updates=159, lr=2.18085e-06, gnorm=0.105, clip=0.000, oom=0.000, loss_scale=8.000, wall=791, train_wall=287, accur| epoch 010: 100%|█| 16/16 [00:28<00:00,  1.99s/it, loss=0.007, nll_loss=0.000, ppl=1.00, wps=709, ups=0, wpb=1282.625, bsz=15.625, num_updates=160, lr=2.12766e-06, gnorm=0.103, clip=0.000, oom=0.000, loss_scale=8.000, wall=793, train_wall=289, accur                                                                                                                                                                                                                                                          | epoch 010 | loss 0.007 | nll_loss 0.000 | ppl 1.00 | wps 708 | ups 0 | wpb 1282.625 | bsz 15.625 | num_updates 160 | lr 2.12766e-06 | gnorm 0.103 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 793 | train_wall 289 | accuracy 1
extra_meters:  defaultdict(<function validate.<locals>.<lambda> at 0x7f26667418c8>, {'accuracy': <fairseq.meters.AverageMeter object at 0x7f2666776978>})
0.9309764309764309
accuracy <fairseq.meters.AverageMeter object at 0x7f2666776978>
f1 <fairseq.meters.AverageMeter object at 0x7f26665b38d0>
| epoch 010 | valid on 'valid' subset | loss 0.275 | nll_loss 0.003 | ppl 1.00 | num_updates 160 | best_f1 0.930976 | accuracy 0.96875 | f1 0.930976
| saved checkpoint ../roberta.large.mnli/checkpoint10.pt (epoch 10 @ 160 updates) (writing took 75.51328825950623 seconds)
| done training in 870.7 seconds